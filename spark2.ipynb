{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.context import SparkContext\n",
    "#from pyspark.ml.classification import LogisticRegression\n",
    "'''Random Forest'''\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.feature import VectorIndexer\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc = SparkContext.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the data using OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('satisfaction_level', 'double'), ('last_evaluation', 'double'), ('number_project', 'int'), ('average_montly_hours', 'int'), ('time_spend_company', 'int'), ('Work_accident', 'int'), ('left', 'int'), ('promotion_last_5years', 'int'), ('sales', 'string'), ('salary', 'string')]\n"
     ]
    }
   ],
   "source": [
    "data_path = \"HR_comma_sep.csv\"\n",
    "dataset = spark.read.options(header=\"true\", parserLib=\"univocity\", inferSchema=\"true\").csv(data_path)\n",
    "cols = dataset.columns\n",
    "print dataset.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "categoricalColumns = [\"sales\", \"salary\"]\n",
    "stages = []\n",
    "\n",
    "for categoricalCol in categoricalColumns: \n",
    "    stringIndexer = StringIndexer(inputCol=categoricalCol, outputCol=categoricalCol+\"Index\")\n",
    "    encoder = OneHotEncoder(inputCol=categoricalCol+\"Index\", outputCol=categoricalCol+\"classVec\")\n",
    "    stages += [stringIndexer, encoder]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "label_string_indexer = StringIndexer(inputCol = \"left\", outputCol = \"label\")\n",
    "stages += [label_string_indexer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "numericColumns = ['number_project',\n",
    "                  'average_montly_hours',\n",
    "                  'time_spend_company',\n",
    "                  'Work_accident', \n",
    "                  'promotion_last_5years']\n",
    "\n",
    "assemblerInputs = map(lambda c: c + \"classVec\", categoricalColumns) + numericColumns\n",
    "assembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\n",
    "\n",
    "stages += [assembler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['satisfaction_level', 'last_evaluation', 'number_project', 'average_montly_hours', 'time_spend_company', 'Work_accident', 'left', 'promotion_last_5years', 'sales', 'salary']\n"
     ]
    }
   ],
   "source": [
    "print dataset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+------------------+---------------+--------------+--------------------+------------------+-------------+----+---------------------+-----+------+\n",
      "|label|            features|satisfaction_level|last_evaluation|number_project|average_montly_hours|time_spend_company|Work_accident|left|promotion_last_5years|sales|salary|\n",
      "+-----+--------------------+------------------+---------------+--------------+--------------------+------------------+-------------+----+---------------------+-----+------+\n",
      "|  1.0|(16,[0,9,11,12,13...|              0.38|           0.53|             2|                 157|                 3|            0|   1|                    0|sales|   low|\n",
      "|  1.0|(16,[0,10,11,12,1...|               0.8|           0.86|             5|                 262|                 6|            0|   1|                    0|sales|medium|\n",
      "|  1.0|(16,[0,10,11,12,1...|              0.11|           0.88|             7|                 272|                 4|            0|   1|                    0|sales|medium|\n",
      "|  1.0|(16,[0,9,11,12,13...|              0.72|           0.87|             5|                 223|                 5|            0|   1|                    0|sales|   low|\n",
      "|  1.0|(16,[0,9,11,12,13...|              0.37|           0.52|             2|                 159|                 3|            0|   1|                    0|sales|   low|\n",
      "|  1.0|(16,[0,9,11,12,13...|              0.41|            0.5|             2|                 153|                 3|            0|   1|                    0|sales|   low|\n",
      "|  1.0|(16,[0,9,11,12,13...|               0.1|           0.77|             6|                 247|                 4|            0|   1|                    0|sales|   low|\n",
      "|  1.0|(16,[0,9,11,12,13...|              0.92|           0.85|             5|                 259|                 5|            0|   1|                    0|sales|   low|\n",
      "|  1.0|(16,[0,9,11,12,13...|              0.89|            1.0|             5|                 224|                 5|            0|   1|                    0|sales|   low|\n",
      "|  1.0|(16,[0,9,11,12,13...|              0.42|           0.53|             2|                 142|                 3|            0|   1|                    0|sales|   low|\n",
      "|  1.0|(16,[0,9,11,12,13...|              0.45|           0.54|             2|                 135|                 3|            0|   1|                    0|sales|   low|\n",
      "|  1.0|(16,[0,9,11,12,13...|              0.11|           0.81|             6|                 305|                 4|            0|   1|                    0|sales|   low|\n",
      "|  1.0|(16,[0,9,11,12,13...|              0.84|           0.92|             4|                 234|                 5|            0|   1|                    0|sales|   low|\n",
      "|  1.0|(16,[0,9,11,12,13...|              0.41|           0.55|             2|                 148|                 3|            0|   1|                    0|sales|   low|\n",
      "|  1.0|(16,[0,9,11,12,13...|              0.36|           0.56|             2|                 137|                 3|            0|   1|                    0|sales|   low|\n",
      "|  1.0|(16,[0,9,11,12,13...|              0.38|           0.54|             2|                 143|                 3|            0|   1|                    0|sales|   low|\n",
      "|  1.0|(16,[0,9,11,12,13...|              0.45|           0.47|             2|                 160|                 3|            0|   1|                    0|sales|   low|\n",
      "|  1.0|(16,[0,9,11,12,13...|              0.78|           0.99|             4|                 255|                 6|            0|   1|                    0|sales|   low|\n",
      "|  1.0|(16,[0,9,11,12,13...|              0.45|           0.51|             2|                 160|                 3|            1|   1|                    1|sales|   low|\n",
      "|  1.0|(16,[0,9,11,12,13...|              0.76|           0.89|             5|                 262|                 5|            0|   1|                    0|sales|   low|\n",
      "+-----+--------------------+------------------+---------------+--------------+--------------------+------------------+-------------+----+---------------------+-----+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline(stages=stages)\n",
    "pipelineModel = pipeline.fit(dataset)\n",
    "dataset = pipelineModel.transform(dataset)\n",
    "selectedcols = [\"label\", \"features\"] + cols\n",
    "dataset = dataset.select(selectedcols)\n",
    "dataset.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10567\n",
      "4432\n"
     ]
    }
   ],
   "source": [
    "(trainingData, testData) = dataset.randomSplit([0.7, 0.3], seed = 100)\n",
    "print trainingData.count()\n",
    "print testData.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation sample with random forest algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pipeline = Pipeline(stages=[featureIndexer, rf])\\n\\nmodelRF = pipeline.fit(trainingData)\\n\\npredictions = modelRF.transform(testData)\\npredictions.printSchema()'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"lr = LogisticRegression(labelCol=\"label\", featuresCol=\"features\", maxIter=1000)\n",
    "lrModel = lr.fit(trainingData)\n",
    "\n",
    "predictions = lrModel.transform(testData)\n",
    "predictions.printSchema()\"\"\"\n",
    "'''featureIndexer =\\\n",
    "    VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=4).fit(trainingData)'''\n",
    "from pyspark.ml.classification import RandomForestClassifier as RF\n",
    "rf = RF(labelCol=\"label\", featuresCol=\"features\",numTrees=3)\n",
    "fit = rf.fit(trainingData)\n",
    "transformed = fit.transform(testData)\n",
    "'''pipeline = Pipeline(stages=[featureIndexer, rf])\n",
    "\n",
    "modelRF = pipeline.fit(trainingData)\n",
    "\n",
    "predictions = modelRF.transform(testData)\n",
    "predictions.printSchema()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+--------------------+\n",
      "|label|prediction|         probability|\n",
      "+-----+----------+--------------------+\n",
      "|  0.0|       1.0|[0.44941686609916...|\n",
      "|  0.0|       1.0|[0.31152456882009...|\n",
      "|  0.0|       1.0|[0.31152456882009...|\n",
      "|  0.0|       1.0|[0.44941686609916...|\n",
      "|  0.0|       1.0|[0.44941686609916...|\n",
      "|  0.0|       1.0|[0.31152456882009...|\n",
      "|  0.0|       1.0|[0.44941686609916...|\n",
      "|  0.0|       1.0|[0.44941686609916...|\n",
      "|  0.0|       0.0|[0.54081240572470...|\n",
      "|  0.0|       0.0|[0.54081240572470...|\n",
      "|  0.0|       0.0|[0.54081240572470...|\n",
      "|  0.0|       0.0|[0.67870470300377...|\n",
      "|  0.0|       0.0|[0.54081240572470...|\n",
      "|  0.0|       0.0|[0.67870470300377...|\n",
      "|  0.0|       0.0|[0.54081240572470...|\n",
      "|  0.0|       0.0|[0.54081240572470...|\n",
      "|  0.0|       0.0|[0.67870470300377...|\n",
      "|  0.0|       0.0|[0.67870470300377...|\n",
      "|  0.0|       0.0|[0.54081240572470...|\n",
      "|  0.0|       0.0|[0.54081240572470...|\n",
      "+-----+----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "('The ROC score is (@numTrees=3): ', 0.9074627446849758)\n",
      "('The PR score is (@numTrees=3): ', 0.9628752869929876)\n"
     ]
    }
   ],
   "source": [
    "selected = transformed.select(\"label\", \"prediction\",  \"probability\")\n",
    "selected.show()\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics as metric\n",
    "results = transformed.select(['probability', 'label'])\n",
    " \n",
    "## prepare score-label set\n",
    "results_collect = results.collect()\n",
    "results_list = [(float(i[0][0]), 1.0-float(i[1])) for i in results_collect]\n",
    "scoreAndLabels = sc.parallelize(results_list)\n",
    " \n",
    "metrics = metric(scoreAndLabels)\n",
    "print(\"The ROC score is (@numTrees=3): \", metrics.areaUnderROC)\n",
    "#print(\"The PR score is (@numTrees=3): \", metrics.areaUnderPR)\n",
    "#print trainingData.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Distribution of Pos and Neg cases of the down-sampled training data are: \\n', [Row(label=0.0, count=8054), Row(label=1.0, count=2513)])\n"
     ]
    }
   ],
   "source": [
    "from numpy.random import randint\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType\n",
    " \n",
    "RATIO_ADJUST = 2.0 ## ratio of pos to neg in the df_subsample\n",
    " \n",
    "counts = trainingData.select('label').groupBy('label').count().collect()\n",
    "higherBound = counts[0][1]\n",
    "TRESHOLD_TO_FILTER = int(RATIO_ADJUST * float(counts[1][1]) / counts[0][1] * higherBound)\n",
    " \n",
    "randGen = lambda x: randint(0, higherBound) if x == 'Positive' else -1\n",
    " \n",
    "udfRandGen = udf(randGen, IntegerType())\n",
    "trainingData = trainingData.withColumn(\"randIndex\", udfRandGen(\"label\"))\n",
    "df_subsample = trainingData.filter(trainingData['randIndex'] < TRESHOLD_TO_FILTER)\n",
    "df_subsample = df_subsample.drop('randIndex')\n",
    " \n",
    "print(\"Distribution of Pos and Neg cases of the down-sampled training data are: \\n\", df_subsample.groupBy(\"label\").count().take(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Random Forest Algorithm with cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Round: ', 0)\n",
      "('The ROC score is (@numTrees=3): ', 0.906965362838021)\n",
      "('Round: ', 1)\n",
      "('The ROC score is (@numTrees=3): ', 0.906965362838025)\n",
      "('Round: ', 2)\n",
      "('The ROC score is (@numTrees=3): ', 0.9069653628380288)\n",
      "('Round: ', 3)\n",
      "('The ROC score is (@numTrees=3): ', 0.906965362838027)\n",
      "('Round: ', 4)\n",
      "('The ROC score is (@numTrees=3): ', 0.9069653628380272)\n",
      "('Round: ', 5)\n",
      "('The ROC score is (@numTrees=3): ', 0.906965362838025)\n",
      "('Round: ', 6)\n",
      "('The ROC score is (@numTrees=3): ', 0.9069653628380273)\n",
      "('Round: ', 7)\n",
      "('The ROC score is (@numTrees=3): ', 0.9069653628380294)\n",
      "('Round: ', 8)\n",
      "('The ROC score is (@numTrees=3): ', 0.9069653628380289)\n",
      "('Round: ', 9)\n",
      "('The ROC score is (@numTrees=3): ', 0.906965362838025)\n"
     ]
    }
   ],
   "source": [
    "from numpy.random import randint\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics as metric\n",
    "import numpy as np\n",
    "RATIO_ADJUST = 3.0 ## ratio of pos to neg in the df_subsample\n",
    "TOTAL_MODELS = 10\n",
    "total_results = None\n",
    "final_result = None\n",
    " \n",
    "#counts = trainingData.select('binary_response').groupBy('binary_response').count().collect()\n",
    "highestBound = counts[0][1]\n",
    "TRESHOLD_TO_FILTER = int(RATIO_ADJUST * float(counts[1][1]) / counts[0][1] * highestBound)\n",
    "## UDF\n",
    "randGen = lambda x: randint(0, highestBound) if x == 'Positive' else -1\n",
    "udfRandGen = udf(randGen, IntegerType())\n",
    " \n",
    "## ensembling\n",
    "for N in range(TOTAL_MODELS):\n",
    "    print(\"Round: \", N)\n",
    "    trainingDataIndexed = trainingData.withColumn(\"randIndex\", udfRandGen(\"label\"))\n",
    "    df_subsample = trainingDataIndexed.filter(trainingDataIndexed['randIndex'] < TRESHOLD_TO_FILTER).drop('randIndex')\n",
    "    ## training and prediction\n",
    "    rf = RF(labelCol='label', featuresCol='features',numTrees=3)\n",
    "    fit = rf.fit(df_subsample)\n",
    "    transformed = fit.transform(testData)\n",
    "    result_pair = transformed.select(['probability', 'label'])\n",
    "    result_pair = result_pair.collect()\n",
    "    this_result = np.array([float(i[0][1]) for i in result_pair])\n",
    "    this_result = list(this_result.argsort().argsort() / (float(len(this_result) + 1)))\n",
    " \n",
    "    ## sum up all the predictions, and average to get final_result\n",
    "    if total_results is None:\n",
    "       total_results = this_result\n",
    "    else:\n",
    "       total_results = [i+j for i, j in zip(this_result, total_results)]\n",
    "    final_result = [i/(N+1) for i in total_results]\n",
    " \n",
    "    results_list = [(float(i), float(j[1])) for i, j in zip(final_result, result_pair)]\n",
    "    scoreAndLabels = sc.parallelize(results_list)\n",
    " \n",
    "    metrics = metric(scoreAndLabels)\n",
    "    print(\"The ROC score is (@numTrees=3): \", metrics.areaUnderROC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
